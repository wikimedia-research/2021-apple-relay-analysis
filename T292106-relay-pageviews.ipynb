{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b924165",
   "metadata": {},
   "source": [
    "# Adoption of Apple's Relay Service\n",
    "\n",
    "After discussing our findings and needs further, [T292106](https://phabricator.wikimedia.org/T292106) got an update to ask the following question:\n",
    "\n",
    "* \"How widespread is the adoption of Relay among Safari users? How has it been changing over time?\"\n",
    "\n",
    "Martin Urbanec wrote [code to measure this on a per-wiki, per-day basis](https://github.com/nettrom/2021-icloud-private-relay-usage). We've used that code as a starting point for our earlier investigation into edits and blocks. In this case, we want to know what scale we can aggregate pageview data on while maintaining a reasonable execution time. Can we do this across all projects on a daily basis? Turns out we can, and that's what this notebook does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d8948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipaddress\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "from wmfdata import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8df175fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = os.environ.get(\"SPARK_HOME\", \"/usr/lib/spark2\")\n",
    "findspark.init(SPARK_HOME)\n",
    "from pyspark.sql import functions as F, types as T, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c200878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "## Using standard wmfdata.spark session settings, but increasing memory overhead because we've seen\n",
    "## tasks erroring out based on that. \n",
    "spark_session = spark.get_session(\n",
    "    app_name = 'nettrom-apple-relay-aggregator',\n",
    "    extra_settings = {'spark.executor.memoryOverhead' : '12G'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40672626",
   "metadata": {},
   "source": [
    "## Load in the Relay Range Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e6b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iCloud's private relay egress ranges\n",
    "# data comes from https://mask-api.icloud.com/egress-ip-ranges.csv\n",
    "relay_ranges = pd.read_csv('datasets/egress-ip-ranges.csv',\n",
    "                           sep=',', names=['range', 'country', 'region', 'city', 'empty']).drop(columns=['empty'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b15dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This data structure is based on using https://stackoverflow.com/a/1004527\n",
    "## to determine if an IP falls within a given network, which gives the following\n",
    "## assertions:\n",
    "## 1: IPv4 and IPV6 networks are disjoint so we can split on IP version. There used to be\n",
    "##    compatibility between the networks, but that was deprecated according to\n",
    "##    https://networkengineering.stackexchange.com/questions/57903/are-the-ipv6-address-space-and-ipv4-address-space-completely-disjoint\n",
    "## 2: The netmask is binary-AND'ed onto the binary IP address, hence\n",
    "##    the second layer are the netmasks, of which we expect there to be a limited number. \n",
    "## 3: We then have a limited set of possible networks which are all numbers\n",
    "##    so we store those as a set and let Python handle it, which gives us fast lookup.\n",
    "\n",
    "dict_nets = {\n",
    "    '4' : defaultdict(set),\n",
    "    '6' : defaultdict(set)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21296070",
   "metadata": {},
   "outputs": [],
   "source": [
    "for net_raw in relay_ranges.range:\n",
    "    net = ipaddress.ip_network(net_raw)\n",
    "    \n",
    "    net_v = str(net.version)\n",
    "    \n",
    "    dict_nets[net_v][net.netmask].add(int(net.network_address))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c37b0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ip_private_relay(ip_raw):\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ip_raw)\n",
    "        bin_ip = int(ip)\n",
    "    \n",
    "        for netmask, range_set in dict_nets[str(ip.version)].items():\n",
    "            bin_netmask = int(netmask)\n",
    "            if (bin_ip & bin_netmask) in range_set:\n",
    "                return(True)\n",
    "    except ValueError: # not a valid IP address\n",
    "        pass \n",
    "    \n",
    "    return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db497c",
   "metadata": {},
   "source": [
    "## Aggregate Viewership Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e07feefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggreate_daily(spark_session, this_day, db_table):\n",
    "    '''\n",
    "    Use `spark_session` to aggregate daily pageviews for `this_day` from\n",
    "    iOS 15 split by project and whether they're using an IP within the relay,\n",
    "    and store that data in `db_table`\n",
    "    '''\n",
    "   \n",
    "    viewers_data = (\n",
    "        spark_session.read.table(\"wmf.pageview_actor\")\n",
    "\n",
    "        .where(F.col(\"year\") == this_day.year)\n",
    "        .where(F.col(\"month\") == this_day.month)\n",
    "        .where(F.col(\"day\") == this_day.day)\n",
    "\n",
    "        # only user pageview traffic\n",
    "        .where(F.col(\"agent_type\") == 'user')\n",
    "        .where(F.col(\"is_pageview\") == True)\n",
    "\n",
    "        # exclude mobile app -- private relay does not affect it\n",
    "        .where(F.col(\"access_method\") != 'mobile app')\n",
    "\n",
    "        # only iOS 15 devices\n",
    "        .where(F.col(\"user_agent_map.os_family\") == 'iOS')\n",
    "        .where(F.col(\"user_agent_map.os_major\") == '15')\n",
    "        \n",
    "    )\n",
    "    viewers_data_is_relay = spark_session.createDataFrame(\n",
    "        viewers_data.rdd.map(lambda r: T.Row(\n",
    "            log_date = dt.date(r.year, r.month, r.day),\n",
    "            project = \"{}.{}\".format(r.normalized_host.project, r.normalized_host.project_family),\n",
    "            is_relay = is_ip_private_relay(r.ip)\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    agg_viewer_data_by_relay = (\n",
    "        viewers_data_is_relay\n",
    "        .groupBy(\"project\", \"log_date\", \"is_relay\")\n",
    "        .agg(F.count(\"*\").alias(\"num_views\"))\n",
    "    )\n",
    "    agg_viewer_data_by_relay.write.insertInto(db_table)\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1cbd735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE nettrom_apple_relay.daily_usage_per_project (\n",
      "    project STRING COMMENT \"project name (e.g. en.wikipedia)\",\n",
      "    log_date DATE COMMENT \"the date for which pageview data is aggregated\",\n",
      "    is_relay BOOLEAN COMMENT \"did the pageviews use a Relay Service IP address?\",\n",
      "    num_views INT COMMENT \"the number of pageviews\"\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_name = 'nettrom_apple_relay'\n",
    "table_name = 'daily_usage_per_project'\n",
    "\n",
    "create_table_statement = f'''\n",
    "CREATE TABLE {db_name}.{table_name} (\n",
    "    project STRING COMMENT \"project name (e.g. en.wikipedia)\",\n",
    "    log_date DATE COMMENT \"the date for which pageview data is aggregated\",\n",
    "    is_relay BOOLEAN COMMENT \"did the pageviews use a Relay Service IP address?\",\n",
    "    num_views INT COMMENT \"the number of pageviews\"\n",
    ")\n",
    "'''\n",
    "\n",
    "# print(create_table_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is expected to run daily as a cron job, at which point we're aggregating\n",
    "## data from the previous day.\n",
    "\n",
    "today = dt.date.today()\n",
    "yesterday = today - dt.timedelta(days = 1)\n",
    "\n",
    "aggreate_daily(spark_session, yesterday, f'{db_name}.{table_name}')\n",
    "print(f'completed daily aggregation for {yesterday}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
